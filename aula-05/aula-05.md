# Aula 5: Criando um sistema para busca em documentos usando embeddings e a Gemini API

Nesta aula, aprendi como utilizar um Large Language Model (LLM) para acessar documentos e como criar embeddings usando o Google Colab.

## Uso de um LLM para acessar documentos

Exploramos como um LLM pode ser utilizado para realizar consultas em documentos, buscando trechos relevantes com base em uma consulta fornecida. Aprendi os seguintes passos:

1. Definição dos documentos: Criamos uma lista de documentos com títulos e conteúdos relevantes.
2. Embedding dos documentos: Utilizamos um modelo de embedding para converter os documentos em representações numéricas.
3. Geração de consulta: Criamos uma função para gerar uma consulta e encontrar o trecho mais relevante nos documentos.
4. Exemplo prático: Demonstramos como usar o LLM para responder a consultas, como "Como faço para trocar marchas em um carro do Google?".

## Criação de embeddings pelo Google Colab

Além disso, aprendi como criar embeddings usando o Google Colab, seguindo estes passos:

1. Importação de bibliotecas e configuração inicial.
2. Exemplo de embedding: Demonstração de como gerar um embedding para um texto de exemplo.
3. Embedding de documentos: Desenvolvimento de uma função para gerar embeddings para cada documento em nossa lista.
4. Busca com embeddings: Desenvolvimento de uma função para buscar o trecho mais relevante com base em uma consulta utilizando embeddings.
5. Geração de texto descontraído: Utilizamos um modelo generativo para reescrever o trecho encontrado de forma mais descontraída.

Acesse [aqui](./Aula-05.ipynb) o arquivo gerado durante a aula.

Com esses aprendizados, agora estou mais capacitado em utilizar LLMs para acessar documentos e criar embeddings usando o Google Colab. Essas habilidades ampliam meu conhecimento em processamento de linguagem natural e me capacitam a trabalhar com ferramentas poderosas de IA.
